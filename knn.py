# -*- coding: utf-8 -*-
"""knn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-NYdfl1h7jtaAUyRq4t1j-VIJ4LBGojd
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

diabetes_data=pd.read_csv('/content/sample_data/diabetes.csv')
diabetes_data.head()

diabetes_data.describe().T



#linear regression

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, classification_report

# Load the dataset (Pima Indians Diabetes dataset as an example)
# Available in sklearn or from UCI Repository
from sklearn.datasets import load_diabetes

# Step 1: Load the dataset
diabetes_data = load_diabetes()
X = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)
y = pd.Series(diabetes_data.target)

# Step 2: Data Preprocessing (optional scaling, handling missing data, etc.)
# For simplicity, we assume the data is clean. Scaling can be done using StandardScaler.

# Step 3: Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Linear Regression Model
linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

# Predicting the results for the test set
y_pred_linear = linear_reg.predict(X_test)

# Evaluate the Linear Regression model
mse = mean_squared_error(y_test, y_pred_linear)
print(f"Mean Squared Error for Linear Regression: {mse}")

# Step 5: Logistic Regression (Assume y is binary for this case)
# Convert the target into binary (e.g., let's assume threshold to classify diabetes)
y_binary = np.where(y > y.median(), 1, 0)  # Simple thresholding

# Split the dataset again for binary classification
X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X, y_binary, test_size=0.2, random_state=42)

# Logistic Regression Model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_bin, y_train_bin)

# Predicting the results for the test set
y_pred_logistic = log_reg.predict(X_test_bin)

# Evaluate the Logistic Regression model
accuracy = accuracy_score(y_test_bin, y_pred_logistic)
print(f"Accuracy for Logistic Regression: {accuracy}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test_bin, y_pred_logistic)}")
print(f"Classification Report:\n{classification_report(y_test_bin, y_pred_logistic)}")

def linear_regression(x, y):

    if len(x) != len(y):
        raise ValueError("x and y must have the same number of elements.")

    n = len(x)


    sum_x = sum(x)
    sum_y = sum(y)
    sum_xy = sum(xi * yi for xi, yi in zip(x, y))
    sum_x2 = sum(xi ** 2 for xi in x)


    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
    b = (sum_y - m * sum_x) / n


    return m, b


x = [1, 2, 3, 4, 5]
y = [2, 4, 5, 4, 5]

m, b = linear_regression(x, y)

print(f"Slope (m): {m}")
print(f"Intercept (b): {b}")


def predict(x_val, m, b):
    return m * x_val + b


x_val = 6
predicted_y = predict(x_val, m, b)
print(f"Predicted value for x = {x_val}: {predicted_y}")

import os
print(os.listdir('/content'))

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import load_iris

# Load the dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the KNN model
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Make predictions
y_pred = knn.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))